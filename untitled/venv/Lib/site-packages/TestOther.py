from urllib import request
import datetime
import re
import os.path
import requests


# define file save function


def save_file(this_download_url, path):
    print("- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ")
    time1 = datetime.datetime.now()
    print(str(time1)[:-7], )
    if os.path.isfile(path):
        file_size = os.path.getsize(path) / 1024 / 1024
        print("File " + path + " (" + str(file_size) + "Mb) already exists.")
        return
    else:
        print("Downloading " + path + "...")
        r = requests.get(this_download_url, stream=True)
        with open(path.encode('utf-8'), "wb") as code:
            code.write(r.content)
        time2 = datetime.datetime.now()
        print(str(time2)[:-7], )
        print(path + " Done.")
        use_time = time2 - time1
        print("Time used: " + str(use_time)[:-7] + ", ", )
        file_size = os.path.getsize(path) / 1024 / 1024
        print(
            "File size: " + str(file_size) + " MB, Speed: " + str(file_size / (use_time.total_seconds()))[:4] + "MB/s")


# define file download url


def download_url(website_url):
    # fuck_you_header = {
    #     'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'}
    proxy_support = request.ProxyHandler({'http': '135.245.48.34:8000'})
    opener = request.build_opener(proxy_support)
    request.install_opener(opener)
    print("pass1")
    # req = request.Request(website_url)
    print("pass2")
    # global max_time
    # max_time = 10
    # global request_status
    # request_status = True
    # for x in range(max_time):
    #     try:
    #         request.urlopen(req, timeout=10).read()
    #         print("pass3")
    #         break
    #     except():
    #         if i < max_time - 1:
    #             continue
    #         else:
    #             print("URLError: <urlopen error timeout> failed for All times")
    #             request_status = False
    # if request_status:
    content = request.urlopen(website_url).read()
    print(content.encode("utf8"))
    pattern = re.compile(r"http://m4.26ts.com/[.0-9-a-zA-Z]*.mp4")
    match = pattern.search(content)

    if match:
        the_url = match.group()
        save_file(the_url, the_url[19:])
    else:
        print("No video found.")
    # else:
    #     print("Sorry,can NOT connect to the URL: " + str(website_url))


# define main method for extract the Video


urls = ["https://www.baidu.com", ]
""" https://corp.kaltura.com/
    http://www.46ek.com/view/22133.html
"""
count = 0
print(len(urls), )
print(" videos to be downloaded...")
for i in urls:
    count += 1
    print(count)
    download_url(i)
print("All done")
